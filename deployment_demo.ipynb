{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wavenet_model import *\n",
    "from audio_data import WavenetDataset\n",
    "\n",
    "dtype = torch.FloatTensor\n",
    "ltype = torch.LongTensor\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('use gpu')\n",
    "    dtype = torch.cuda.FloatTensor\n",
    "    ltype = torch.cuda.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WaveNetModel(layers=10,\n",
    "                     blocks=3,\n",
    "                     dilation_channels=32,\n",
    "                     residual_channels=32,\n",
    "                     skip_channels=1024,\n",
    "                     end_channels=512, \n",
    "                     output_length=16,\n",
    "                     dtype=dtype, \n",
    "                     bias=True)\n",
    "model = load_latest_model_from('snapshots', use_cuda=use_cuda)\n",
    "\n",
    "model.dtype = dtype\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "else:\n",
    "    model.cpu()\n",
    "    \n",
    "print('model: ', model)\n",
    "print('receptive field: ', model.receptive_field)\n",
    "print('parameter count: ', model.parameter_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = WavenetDataset(dataset_file='train_samples/bach_chaconne/dataset.npz',\n",
    "                      item_length=model.receptive_field + model.output_length - 1,\n",
    "                      target_length=model.output_length,\n",
    "                      file_location='train_samples/bach_chaconne',\n",
    "                      test_stride=500)\n",
    "print('the dataset has ' + str(len(data)) + ' items')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import onnx\n",
    "import shutil\n",
    "from timeit import default_timer as timer\n",
    "import vai_q_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_data = data[260000][0] # use start data from the data set\n",
    "start_data = torch.max(start_data, 0)[1] # convert one hot vectors to integers\n",
    "\n",
    "first_samples = start_data\n",
    "input_data = Variable(torch.FloatTensor(1, model.classes, 1).zero_())\n",
    "input_data = input_data.scatter_(1, first_samples[0:1].view(1, -1, 1), 1.)\n",
    "\n",
    "start = timer()\n",
    "for _ in range(1000):\n",
    "    model(input_data)\n",
    "pytorch_total = timer() - start\n",
    "\n",
    "print(f\"Inference Time: {pytorch_total / 1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prog_callback(step, total_steps):\n",
    "    print(str(100 * step // total_steps) + \"% generated\")\n",
    "\n",
    "start = timer()\n",
    "generated = model.generate_fast(num_samples=160000,\n",
    "                                 first_samples=start_data,\n",
    "                                 progress_callback=prog_callback,\n",
    "                                 progress_interval=1000,\n",
    "                                 temperature=1.0,\n",
    "                                 regularize=0.)\n",
    "pytorch_total = timer() - start\n",
    "\n",
    "print(f\"Generation Time: {pytorch_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Runtime (CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep for ONNX export\n",
    "inputs = {\"x\": torch.rand(1, 256, 1)}\n",
    "input_names = ['input']\n",
    "output_names = ['output']\n",
    "dynamic_axes = {'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    "model_path = \"models/wavenet.onnx\"\n",
    "\n",
    "# Call export function\n",
    "torch.onnx.export(\n",
    "        model,\n",
    "        inputs,\n",
    "        model_path,\n",
    "        export_params=True,\n",
    "        opset_version=13,  # Recommended opset\n",
    "        input_names=input_names,\n",
    "        output_names=output_names,\n",
    "        dynamic_axes=dynamic_axes,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the quantized ONNZ Model\n",
    "model_path = r'./models/wavenet.onnx'\n",
    "onnx_model = onnx.load(model_path)\n",
    "\n",
    "cpu_options = onnxruntime.SessionOptions()\n",
    "\n",
    "# Create Inference Session to run the quantized model on the CPU\n",
    "cpu_session = onnxruntime.InferenceSession(\n",
    "    onnx_model.SerializeToString(),\n",
    "    providers = ['CPUExecutionProvider'],\n",
    "    sess_options=cpu_options,\n",
    ")\n",
    "\n",
    "# Run Inference\n",
    "start = timer()\n",
    "for _ in range(1000):\n",
    "    cpu_results = cpu_session.run(None, {})\n",
    "cpu_total = timer() - start\n",
    "\n",
    "print(f\"Inference Time: {cpu_total / 1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fast(model,\n",
    "                    num_samples,\n",
    "                    first_samples=None,\n",
    "                    temperature=1.,\n",
    "                    regularize=0.,\n",
    "                    progress_callback=None,\n",
    "                    progress_interval=100,\n",
    "                    session=None):\n",
    "    model.eval()\n",
    "    if first_samples is None:\n",
    "        first_samples = torch.LongTensor(1).zero_() + (model.classes // 2)\n",
    "    first_samples = Variable(first_samples)\n",
    "\n",
    "    # reset queues\n",
    "    for queue in model.dilated_queues:\n",
    "        queue.reset()\n",
    "\n",
    "    num_given_samples = first_samples.size(0)\n",
    "    total_samples = num_given_samples + num_samples\n",
    "\n",
    "    input = Variable(torch.FloatTensor(1, model.classes, 1).zero_())\n",
    "    input = input.scatter_(1, first_samples[0:1].view(1, -1, 1), 1.)\n",
    "\n",
    "    # fill queues with given samples\n",
    "    for i in range(num_given_samples - 1):\n",
    "        x = torch.tensor(session.run(None, {})[0])\n",
    "        input.zero_()\n",
    "        input = input.scatter_(1, first_samples[i + 1:i + 2].view(1, -1, 1), 1.).view(1, model.classes, 1)\n",
    "\n",
    "        # progress feedback\n",
    "        if i % progress_interval == 0:\n",
    "            if progress_callback is not None:\n",
    "                progress_callback(i, total_samples)\n",
    "\n",
    "    # generate new samples\n",
    "    generated = np.array([])\n",
    "    regularizer = torch.pow(Variable(torch.arange(model.classes)) - model.classes / 2., 2)\n",
    "    regularizer = regularizer.squeeze() * regularize\n",
    "    tic = time.time()\n",
    "    for i in range(num_samples):\n",
    "        x = torch.tensor(session.run(None, {})[0]).squeeze()\n",
    "\n",
    "        x -= regularizer\n",
    "\n",
    "        if temperature > 0:\n",
    "            # sample from softmax distribution\n",
    "            x /= temperature\n",
    "            prob = F.softmax(x, dim=0)\n",
    "            prob = prob.cpu()\n",
    "            np_prob = prob.data.numpy()\n",
    "            x = np.random.choice(model.classes, p=np_prob)\n",
    "            x = np.array([x])\n",
    "        else:\n",
    "            # convert to sample value\n",
    "            x = torch.max(x, 0)[1][0]\n",
    "            x = x.cpu()\n",
    "            x = x.data.numpy()\n",
    "\n",
    "        o = (x / model.classes) * 2. - 1\n",
    "        generated = np.append(generated, o)\n",
    "\n",
    "        # set new input\n",
    "        x = Variable(torch.from_numpy(x).type(torch.LongTensor))\n",
    "        input.zero_()\n",
    "        input = input.scatter_(1, x.view(1, -1, 1), 1.).view(1, model.classes, 1)\n",
    "\n",
    "        if (i+1) == 100:\n",
    "            toc = time.time()\n",
    "            print(\"one generating step does take approximately \" + str((toc - tic) * 0.01) + \" seconds)\")\n",
    "\n",
    "        # progress feedback\n",
    "        if (i + num_given_samples) % progress_interval == 0:\n",
    "            if progress_callback is not None:\n",
    "                progress_callback(i + num_given_samples, total_samples)\n",
    "\n",
    "    model.train()\n",
    "    mu_gen = mu_law_expansion(generated, model.classes)\n",
    "    return mu_gen\n",
    "\n",
    "start = timer()\n",
    "generated = generate_fast(model=model,\n",
    "                          num_samples=160000,\n",
    "                          first_samples=start_data,\n",
    "                          progress_callback=prog_callback,\n",
    "                          progress_interval=1000,\n",
    "                          temperature=1.0,\n",
    "                          regularize=0.,\n",
    "                          session=cpu_session)\n",
    "cpu_total = timer() - start\n",
    "\n",
    "print(f\"Generation Time: {cpu_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ONNX Runtime (NPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point to the config file path used for the VitisAI Execution Provider\n",
    "config_file_path = \"vaip_config.json\"\n",
    "\n",
    "aie_options = onnxruntime.SessionOptions()\n",
    "\n",
    "aie_session = onnxruntime.InferenceSession(\n",
    "    onnx_model.SerializeToString(),\n",
    "    providers=['VitisAIExecutionProvider'],\n",
    "    sess_options=aie_options,\n",
    "    provider_options = [{'config_file': config_file_path}]\n",
    ")\n",
    "\n",
    "# Run Inference\n",
    "start = timer()\n",
    "for _ in range(1000):\n",
    "    npu_results = aie_session.run(None, {})\n",
    "npu_total = timer() - start\n",
    "\n",
    "print(f\"Inference Time: {npu_total / 1000}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = timer()\n",
    "generated = generate_fast(model=model,\n",
    "                          num_samples=160000,\n",
    "                          first_samples=start_data,\n",
    "                          progress_callback=prog_callback,\n",
    "                          progress_interval=1000,\n",
    "                          temperature=1.0,\n",
    "                          regularize=0.,\n",
    "                          session=aie_session)\n",
    "npu_total = timer() - start\n",
    "\n",
    "print(f\"Generation Time: {npu_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "ipd.Audio(generated, rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wavenet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
